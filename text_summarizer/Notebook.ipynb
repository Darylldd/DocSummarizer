{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018cd1e0-2760-4482-a125-4a66f69afa14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60e16e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Summarize this document 1. NUMBER ONE\n",
      "\n",
      "Globalism can be broadly defined as the interconnectedness of various nations across continents, bringing them closer economically, socially, culturally, and informationally. It promotes deeper integration and cooperation among countries.”\n",
      "\n",
      "\n",
      "2. NUMBER TWO\n",
      "\n",
      "“Economic globalism refers to the integration of national economies into a global one. Over the last few decades, major events like China’s rise and the Soviet Union’s collapse have accelerated globalization. A key driver has been Information and Communication Technology, which has increased both the speed and reach of economic activities.”\n",
      "\n",
      "3. NUMBER THREE\n",
      "\n",
      "The Information and Communication Technology (ICT) revolution has been crucial to globalization. It functions as a general-purpose technology, enhancing productivity and fostering economic growth. ICT has also contributed to the emergence of the ‘Information Economy,’ where innovation drives the transformation of products and services. The United States alone accounts for over half—approximately 75 percent!† Of these inventions worldwide by means that involve personal or professional knowledge in some way —– more than 70% are based on consumer goods such .¹№conomics.\" This is what we call an Internet economy: \"a system whose primary purpose becomes direct access from outside sources without being tied up with governmental control [and] relying solely upon individual people's willingness rather then their own efforts,\" writes John Wrenberg at Harvard University Press today.[1][note 2](www..),[4]. In other words., \"[i.]f our institutions do not become centralized online via government oversight while maintaining public trust within private organizations,[5], they may lose influence\" if society lacks open standards related­tly associated closely those technologies found inside American businesses.\"[6]). Moreover , it must therefore remain decentralized because there will always eventually come new markets through commercialization – i.) existing competitors' technological capacities (\"eCommerce\") but competing companies cannot afford any higher costs; ii.—these competition forces compete globally versus local ones even though consumers generally want cheaper solutions.(citations omitted). Furthermore,, despite widespread evidence stating otherwise during recent years concerning how ecommerce operates under UU regulation since 2001/02(including testimony before Congress)(also see EIA 2012):  the most important factor leading toward deregulation was industry monopoly status when federal rules were written about 2004:[7],[8]) But only recently did regulators realize its importance after reviewing hundredsof regulatory reports including many published here…. We need action now so much greater certainty regarding future opportunities available… For example … New laws governing internet regulations could mean huge changes indeed…\" So why would anyone believe financial intermediaries should regulate everything? Is digital currency illegal?\" Here again wunderkind Paul Krugman seems too clueless nor ignorant who believes money laundering doesn't exist until you read his analysis below […] http://blogs...\n",
      "\n",
      "Key Insights:\n",
      "1 (CARDINAL)\n",
      "ONE (CARDINAL)\n",
      "2 (CARDINAL)\n",
      "TWO (CARDINAL)\n",
      "the last few decades (DATE)\n",
      "China (GPE)\n",
      "the Soviet Union’s (GPE)\n",
      "Information and Communication Technology (ORG)\n",
      "3 (CARDINAL)\n",
      "THREE (CARDINAL)\n",
      "ICT (ORG)\n",
      "ICT (ORG)\n",
      "the ‘Information Economy (ORG)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import spacy\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from pathlib import Path\n",
    "import fitz  # PyMuPDF\n",
    "import docx  # python-docx\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set pad_token to eos_token as GPT-2 lacks a padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the CNN/Daily Mail dataset for fallback summaries\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "# Load spaCy model for NER (Named Entity Recognition)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        doc = fitz.open(file_path)\n",
    "        text = \"\".join([page.get_text() for page in doc])\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return f\"Error reading PDF: {str(e)}\"\n",
    "\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        doc = docx.Document(file_path)\n",
    "        text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return f\"Error reading DOCX: {str(e)}\"\n",
    "\n",
    "def summarize_with_gpt2(text, prompt=None):\n",
    "    \"\"\"\n",
    "    Summarizes the input text using GPT-2. If no text is provided, it uses a default article from the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - text (str): The input text to summarize.\n",
    "    - prompt (str): An optional prompt to guide the summarization.\n",
    "\n",
    "    Returns:\n",
    "    - summary (str): The generated summary.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        default_article = dataset[\"train\"][0][\"article\"]\n",
    "        text = default_article if not prompt else prompt + \" \" + default_article\n",
    "    else:\n",
    "        if prompt:\n",
    "            text = prompt + \" \" + text\n",
    "\n",
    "    # Tokenize and encode the text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True).to(device)\n",
    "\n",
    "    # Generate the summary\n",
    "    max_new_tokens = 1024\n",
    "    input_length = inputs['input_ids'].shape[-1]\n",
    "    max_tokens = min(input_length + max_new_tokens, 1024)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=max_tokens,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=1,\n",
    "        temperature=0.8,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "def extract_key_insights(text):\n",
    "    \"\"\"\n",
    "    Extract key insights from the text using spaCy (Named Entity Recognition).\n",
    "    \n",
    "    Parameters:\n",
    "    - text (str): The input text to analyze.\n",
    "\n",
    "    Returns:\n",
    "    - insights (list): A list of key insights (named entities).\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    insights = []\n",
    "    for ent in doc.ents:\n",
    "        insights.append({'text': ent.text, 'label': ent.label_})\n",
    "    \n",
    "    return insights\n",
    "\n",
    "# Example file processing\n",
    "\n",
    "def process_file(file_path, prompt=\"\"):\n",
    "    \"\"\"\n",
    "    Main function to process a file, extract text, summarize, and extract key insights.\n",
    "    \"\"\"\n",
    "    # Extract text based on file extension\n",
    "    file_extension = file_path.split('.')[-1].lower()\n",
    "    \n",
    "    if file_extension == 'pdf':\n",
    "        file_content = extract_text_from_pdf(file_path)\n",
    "    elif file_extension == 'docx':\n",
    "        file_content = extract_text_from_docx(file_path)\n",
    "    elif file_extension == 'txt':\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                file_content = f.read()\n",
    "        except Exception as e:\n",
    "            file_content = f\"Error reading TXT file: {str(e)}\"\n",
    "    else:\n",
    "        file_content = \"Unsupported file type. Please upload a .txt, .pdf, or .docx file.\"\n",
    "    \n",
    "    # Generate summary and key insights if file content is valid\n",
    "    if \"Error\" not in file_content:\n",
    "        summary = summarize_with_gpt2(file_content, prompt)\n",
    "        insights = extract_key_insights(file_content)\n",
    "        \n",
    "        return summary, insights\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Example usage\n",
    "file_path = r'C:\\Users\\oumki\\OneDrive\\Documents\\Test.txt'\n",
    "summary, insights = process_file(file_path, prompt=\"Summarize this document\")\n",
    "\n",
    "# Print the results\n",
    "if summary:\n",
    "    print(\"Summary:\")\n",
    "    print(summary)\n",
    "    print(\"\\nKey Insights:\")\n",
    "    for insight in insights:\n",
    "        print(f\"{insight['text']} ({insight['label']})\")\n",
    "else:\n",
    "    print(\"Error processing file.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
